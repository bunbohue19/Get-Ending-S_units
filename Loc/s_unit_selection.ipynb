{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import javalang\n",
    "import numpy as np\n",
    "\n",
    "data = open(\"./dataset/full_set.json\", \"r\")\n",
    "\n",
    "data_dict = json.load(data)\n",
    "# data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_source(src):\n",
    "    return src.split(\"\\n\")\n",
    "\n",
    "def view_source(src):\n",
    "    newSrc = src.splitlines()\n",
    "    for s_unit in newSrc:\n",
    "        print(s_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sample = data_dict[1]['source'] \n",
    "# view_source(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_source(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = list(javalang.tokenizer.tokenize(test_sample))\n",
    "\n",
    "# for token in tokens:\n",
    "#     print(f\"Token: '{token.value}', position: {token.position}, is a {type(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in tokens:\n",
    "#     if isinstance(token, javalang.tokenizer.Keyword):\n",
    "#         print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['return x;']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Get the unique values and remove '}' character\n",
    "\"\"\"\n",
    "def unique(ending_s_units):\n",
    "    ending_s_units = [ending_s_unit for ending_s_unit in ending_s_units if ending_s_unit != '}']\n",
    "    return np.unique(np.array(ending_s_units))\n",
    "\n",
    "\"\"\"\n",
    "    Returns type of the method\n",
    "\"\"\"\n",
    "def get_method_type(src):\n",
    "    # Add fake class in order to avoid error :D\n",
    "    code_snippet = 'class A { \\n' + src + '\\n}'\n",
    "    tree = javalang.parse.parse(code_snippet)\n",
    "    for _, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "        if node.return_type is None:\n",
    "            return 'void'\n",
    "        else:\n",
    "            return node.return_type.name\n",
    "\n",
    "\"\"\"\n",
    "    Get the list of keywords\n",
    "\"\"\"\n",
    "def get_keywords(src):\n",
    "   tokens = list(javalang.tokenizer.tokenize(src))\n",
    "   return [token.value for token in tokens if isinstance(token, javalang.tokenizer.Keyword)]\n",
    "\n",
    "\"\"\"\n",
    "    Returns the Ending S_units.\n",
    "\"\"\"\n",
    "def get_ending_s_units(src):\n",
    "    # tokens = list(javalang.tokenizer.tokenize(src))\n",
    "    # keywords = get_keywords(src)\n",
    "    s_units = read_source(src)\n",
    "    num_of_lines = len(s_units)\n",
    "    method_type = get_method_type(src)\n",
    "    \n",
    "    # For non-void return type\n",
    "    inner_ending_s_units = []\n",
    "    \n",
    "    # For void return type\n",
    "    outer_ending_s_unit = \"\"\n",
    "    \n",
    "    match method_type:\n",
    "        case 'void':\n",
    "            for i in range(num_of_lines):\n",
    "                if 'if' or 'else' or 'else if' or 'try' or 'catch' or 'finally' or 'switch' in s_units[i]:\n",
    "                    for j in range(i, num_of_lines):\n",
    "                        if '}' in s_units[j]:\n",
    "                            inner_ending_s_units.append(s_units[j - 1].strip())\n",
    "                            break\n",
    "            for k in reversed(range(num_of_lines)):\n",
    "                if '}' not in s_units[k]:\n",
    "                    outer_ending_s_unit = s_units[k].strip()\n",
    "                    break\n",
    "        case _:\n",
    "            for i in range(num_of_lines):\n",
    "                if 'return' in s_units[i]:\n",
    "                    inner_ending_s_units.append(s_units[i].strip())\n",
    "        \n",
    "    if method_type == 'void':\n",
    "        for inner in inner_ending_s_units:\n",
    "            if outer_ending_s_unit == inner:\n",
    "                return outer_ending_s_unit\n",
    "        \n",
    "    return unique(inner_ending_s_units)      \n",
    "\n",
    "# Test sample\n",
    "src = data_dict[25]['source']\n",
    "\n",
    "# Get method type\n",
    "# print(get_method_type(src))\n",
    "\n",
    "# Get keywords\n",
    "# print(get_keywords(src))\n",
    "\n",
    "# Get Ending S Units\n",
    "print(get_ending_s_units(src))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
